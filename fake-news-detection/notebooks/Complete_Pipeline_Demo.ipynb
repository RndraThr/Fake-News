{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection - Complete KDD Pipeline\n",
    "\n",
    "Implementasi lengkap metodologi KDD sesuai dokumen penelitian:\n",
    "1. Data Selection\n",
    "2. Data Preprocessing (5 tahap)\n",
    "3. Feature Extraction (TF-IDF)\n",
    "4. Model Training (Logistic Regression)\n",
    "5. Pattern Evaluation (Confusion Matrix + Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "print(\"Importing custom modules...\")\n",
    "from preprocessing import TextPreprocessor\n",
    "from tfidf_calculator import CustomTFIDFCalculator\n",
    "from model import FakeNewsLogisticRegression\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Selection Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"=== DATA SELECTION ===\")\n",
    "train_df = pd.read_csv('../data/raw/train.csv', sep=';', on_bad_lines='skip', encoding='utf-8')\n",
    "\n",
    "print(f\"Total records: {len(train_df)}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Label distribution\n",
    "label_dist = train_df['label'].value_counts()\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  REAL news (0): {label_dist.get(0, 0)} ({label_dist.get(0, 0)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"  FAKE news (1): {label_dist.get(1, 0)} ({label_dist.get(1, 0)/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Use sample untuk demo\n",
    "SAMPLE_SIZE = 2000\n",
    "train_sample = train_df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "print(f\"\\nUsing sample of {SAMPLE_SIZE} records for demonstration\")\n",
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Phase\n",
    "\n",
    "5 tahap preprocessing sesuai dokumen:\n",
    "1. Case Folding\n",
    "2. Punctuation Removal\n",
    "3. Tokenization\n",
    "4. Stopword Removal  \n",
    "5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Demo preprocessing dengan satu sample\n",
    "sample_text = train_sample.iloc[0]['text']\n",
    "print(\"=== SAMPLE PREPROCESSING DEMO ===\")\n",
    "print(f\"Original: {sample_text[:200]}...\")\n",
    "print()\n",
    "\n",
    "# Step by step preprocessing\n",
    "step1 = preprocessor.case_folding(sample_text)\n",
    "print(f\"1. Case Folding: {step1[:200]}...\")\n",
    "\n",
    "step2 = preprocessor.remove_punctuation(step1)\n",
    "print(f\"2. Punctuation Removal: {step2[:200]}...\")\n",
    "\n",
    "step3 = preprocessor.tokenize(step2)\n",
    "print(f\"3. Tokenization: {step3[:20]}...\")\n",
    "\n",
    "step4 = preprocessor.remove_stopwords(step3)\n",
    "print(f\"4. Stopword Removal: {step4[:20]}...\")\n",
    "\n",
    "step5 = preprocessor.stemming(step4)\n",
    "print(f\"5. Stemming: {step5[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess entire sample dataset\n",
    "print(\"\\n=== PREPROCESSING ENTIRE SAMPLE ===\")\n",
    "processed_sample = preprocessor.preprocess_dataset(train_sample, text_column='text')\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nProcessed dataset shape: {processed_sample.shape}\")\n",
    "print(f\"Processed dataset columns: {list(processed_sample.columns)}\")\n",
    "\n",
    "# Sample processed data\n",
    "processed_sample[['text', 'processed_text', 'label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Phase (TF-IDF)\n",
    "\n",
    "Menggunakan rumus TF-IDF dari dokumen:\n",
    "- TF(t,d) = 1 + 10*log(f(t,d))\n",
    "- IDF(t) = 10*log(n/df(t))\n",
    "- Normalisasi: w(t,d) / sqrt(sum(w(t,d)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF calculator\n",
    "tfidf_calc = CustomTFIDFCalculator()\n",
    "\n",
    "# Prepare documents for TF-IDF\n",
    "documents = processed_sample['processed_tokens'].tolist()\n",
    "labels = processed_sample['label'].values\n",
    "\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"Sample document: {documents[0][:10]}...\")\n",
    "\n",
    "# Calculate TF-IDF matrix\n",
    "print(\"\\n=== CALCULATING TF-IDF MATRIX ===\")\n",
    "tfidf_matrix = tfidf_calc.calculate_tfidf_matrix(documents)\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_calc.get_feature_names())}\")\n",
    "print(f\"Vocabulary sample: {tfidf_calc.get_feature_names()[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training Phase (Logistic Regression)\n",
    "\n",
    "Menggunakan sigmoid function: sigmoid(z) = 1 / (1 + e^(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Initialize and train model\n",
    "model = FakeNewsLogisticRegression()\n",
    "training_result = model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training accuracy: {training_result['train_accuracy']:.4f}\")\n",
    "print(f\"Test accuracy: {training_result['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pattern Evaluation Phase\n",
    "\n",
    "Evaluasi menggunakan confusion matrix dan metrik sesuai rumus dokumen:\n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN) \n",
    "- F1-Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Generate comprehensive evaluation report\n",
    "metrics = evaluator.print_evaluation_report(y_test, y_pred, model_name=\"Logistic Regression\")\n",
    "\n",
    "# Compare with previous research\n",
    "evaluator.compare_with_research(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo Prediction\n",
    "\n",
    "Test model dengan contoh teks baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts untuk demo\n",
    "demo_texts = [\n",
    "    \"Vaksin COVID-19 mengandung chip 5G yang bisa mengontrol pikiran manusia! Jangan divaksin karena berbahaya!\",\n",
    "    \"Pemerintah mengumumkan program vaksinasi COVID-19 gratis untuk seluruh warga negara sesuai protokol WHO.\",\n",
    "    \"BREAKING: Ilmuwan menemukan obat ajaib yang bisa menyembuhkan semua penyakit dalam 1 hari!\",\n",
    "    \"Menteri Kesehatan melaporkan penurunan kasus COVID-19 sebesar 15% dalam minggu ini berdasarkan data resmi.\"\n",
    "]\n",
    "\n",
    "print(\"=== DEMO PREDICTIONS ===\")\n",
    "\n",
    "for i, text in enumerate(demo_texts, 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_tokens = preprocessor.preprocess_text(text)\n",
    "    \n",
    "    # Transform to TF-IDF\n",
    "    tfidf_vector = tfidf_calc.transform_new_document(processed_tokens)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(tfidf_vector)[0]\n",
    "    probability = model.predict_proba(tfidf_vector)[0]\n",
    "    \n",
    "    result = \"FAKE\" if prediction == 1 else \"REAL\"\n",
    "    confidence = probability[prediction] * 100\n",
    "    \n",
    "    print(f\"Prediction: {result}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(f\"Probabilities: REAL={probability[0]*100:.2f}%, FAKE={probability[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Analysis\n",
    "\n",
    "Analisis koefisien model dan fitur penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model coefficients\n",
    "coeffs = model.get_model_coefficients()\n",
    "\n",
    "print(\"=== MODEL ANALYSIS ===\")\n",
    "print(f\"Intercept (β₀): {coeffs['intercept']:.4f}\")\n",
    "print(f\"Number of features: {len(coeffs['coefficients'])}\")\n",
    "\n",
    "# Top positive coefficients (indicating FAKE)\n",
    "feature_names = tfidf_calc.get_feature_names()\n",
    "coeff_importance = list(zip(feature_names, coeffs['coefficients']))\n",
    "coeff_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for i, (feature, coeff) in enumerate(coeff_importance[:10]):\n",
    "    direction = \"FAKE\" if coeff > 0 else \"REAL\"\n",
    "    print(f\"{i+1:2d}. {feature:<15} : {coeff:8.4f} ({direction})\")\n",
    "\n",
    "# Demonstration of manual sigmoid calculation\n",
    "print(\"\\n=== MANUAL SIGMOID DEMONSTRATION ===\")\n",
    "sample_z_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "for z in sample_z_values:\n",
    "    sigmoid_val = model.sigmoid(z)\n",
    "    classification = \"FAKE\" if sigmoid_val > 0.5 else \"REAL\"\n",
    "    print(f\"Z = {z:4.1f} → Sigmoid = {sigmoid_val:.4f} → {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pipeline KDD lengkap telah berhasil diimplementasi sesuai metodologi dalam dokumen penelitian:\n",
    "\n",
    "1. ✅ **Data Selection**: Dataset berhasil dimuat dan dieksplorasi\n",
    "2. ✅ **Data Preprocessing**: 5 tahap preprocessing sesuai dokumen\n",
    "3. ✅ **Feature Extraction**: TF-IDF menggunakan rumus dari dokumen\n",
    "4. ✅ **Model Training**: Logistic Regression dengan sigmoid function\n",
    "5. ✅ **Pattern Evaluation**: Confusion matrix dan metrik evaluasi\n",
    "\n",
    "Model siap untuk digunakan dalam aplikasi fake news detection!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}